{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a3b455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d699952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "from IPython.display import Audio\n",
    "from torchaudio.transforms import Fade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d43e804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.PM_Unet import Model_Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6c37106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_sources(\n",
    "        model,\n",
    "        mix,\n",
    "        segment=10.,\n",
    "        sample_rate = 44100,\n",
    "        overlap=0.1,\n",
    "        device='cpu',\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply model to a given mixture. Use fade, and add segments together in order to add model segment by segment.\n",
    "\n",
    "    Args:\n",
    "        segment (int): segment length in seconds\n",
    "        device (torch.device, str, or None): if provided, device on which to\n",
    "            execute the computation, otherwise `mix.device` is assumed.\n",
    "            When `device` is different from `mix.device`, only local computations will\n",
    "            be on `device`, while the entire tracks will be stored on `mix.device`.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = mix.device\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "\n",
    "    batch, channels, length = mix.shape\n",
    "\n",
    "    chunk_len = int(sample_rate * segment * (1 + overlap))\n",
    "    start = 0\n",
    "    end = chunk_len\n",
    "    overlap_frames = overlap * sample_rate\n",
    "    fade = Fade(fade_in_len=0, fade_out_len=int(overlap_frames), fade_shape='linear')\n",
    "\n",
    "    final = torch.zeros(batch, len(['drums', 'bass', 'other', 'vocals']), channels, length, device=device)\n",
    "\n",
    "    while start < length - overlap_frames:\n",
    "        chunk = mix[:, :, start:end]\n",
    "        with torch.no_grad():\n",
    "            out = model.forward(chunk)\n",
    "        out = fade(out)\n",
    "        final[:, :, :, start:end] += out\n",
    "        if start == 0:\n",
    "            fade.fade_in_len = int(overlap_frames)\n",
    "            start += int(chunk_len - overlap_frames)\n",
    "        else:\n",
    "            start += chunk_len\n",
    "        end += chunk_len\n",
    "        if end >= length:\n",
    "            fade.fade_out_len = 0\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a9ed36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def track(SAMPLE_SONG):\n",
    "    segment = 7\n",
    "    overlap = 0.2\n",
    "    \n",
    "    waveform, sample_rate = torchaudio.load(SAMPLE_SONG) \n",
    "    mixture = waveform[:, sample_rate*30: sample_rate*37]\n",
    "    \n",
    "    ref = waveform.mean(0)\n",
    "    waveform = (waveform - ref.mean()) / ref.std()  # normalization\n",
    "\n",
    "    sources = separate_sources(\n",
    "    model,\n",
    "    mixture[None],\n",
    "    segment=segment,\n",
    "    overlap=overlap,\n",
    "    )\n",
    "    sources = sources * ref.std() + ref.mean()\n",
    "\n",
    "    sources_list = ['drums', 'bass', 'other', 'vocals']\n",
    "    B, S, C, T = sources.shape\n",
    "    sources = (sources.view(B, S*C,T)/sources.view(B, S*C,T).max(dim=2)[0].unsqueeze(-1)).view(B, S,C,T)\n",
    "    sources = list(sources)\n",
    "    \n",
    "    audios = dict(zip(sources_list, sources[0]))\n",
    "    audios['original'] = waveform\n",
    "    return audios\n",
    "\n",
    "sample_rate = 44100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3eec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model_Unet(source=['drums', 'bass', 'other', 'vocals'], depth=4, channel=28)\n",
    "model.load_state_dict(torch.load('model_weight_LSTM.pt', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276fd276",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_track = '' + '.wav'\n",
    "audios = track(\"path_track\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audios['vocals'], rate=44100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
